

<!-- a Getting Started Guide and
Step-by-Step Instructions for how you propose to evaluate your artifact (with appropriate connections to the relevant sections of your paper);
The Getting Started Guide should contain setup instructions (including, for example, a pointer to the VM player software, its version, passwords if needed, etc.) and basic testing of your artifact that you expect a reviewer to be able to complete in 30 minutes. Reviewers will follow all the steps in the guide during an initial kick-the-tires phase. The Getting Started Guide should be as simple as possible, and yet it should stress the key elements of your artifact. Anyone who has followed the Getting Started Guide should have no technical difficulties with the rest of your artifact.

The Step by Step Instructions explain how to reproduce any experiments or other activities that support the conclusions in your paper. Write this for readers who have a deep interest in your work and are studying it to improve it or compare against it. If your artifact runs for more than a few minutes, point this out, note how long it is expected to run (roughly) and explain how to run it on smaller inputs. Reviewers may choose to run on smaller inputs or larger inputs depending on available hardware.

Where appropriate, include descriptions of and links to files (included in the archive) that represent expected outputs (e.g., the log files expected to be generated by your tool on the given inputs); if there are warnings that are safe to be ignored, explain which ones they are.

The artifact’s documentation should include the following:

A list of claims from the paper supported by the artifact, and how/why.
A list of claims from the paper not supported by the artifact, and why not.
Example: Performance claims cannot be reproduced in VM, authors are not allowed to redistribute specific benchmarks, etc. Artifact reviewers can then center their reviews / evaluation around these specific claims, though the reviewers will still consider whether the provided evidence is adequate to support claims that the artifact works. -->

# Ruler

Ruler is a framework that uses equality saturation
 to automatically infer rewrite rules for a domain, given an interpreter.
We have used Ruler for a variety of domains including
 booleans,
 bitvectors,
 rationals,
 floats,
 integers, and
 strings.

## Overview

What is the paper about?
Rewrite rules are used in a panoply of 
applications, ...
We present a framework for creating small, expressive rulesets 
through equality saturation. 
To validate our claims, we have implemented a tool
called Ruler.
Currently, Ruler supports several different domains,
with an extensible interface for users to synthesize 
rules for any domain for which they can write an interpreter.



## Claims
- We synthesize powerful and compact rulesets that can be directly used in applications (esp. eqsat applications)
- Our contributions are valuable to this process
    - Configurations for choosing rules
    - choose_eqs
    - What to put in the cvecs

Claims from paper:
Performance. Does Ruler synthesize rewrite rules quickly compared to other approaches?
Can't examine absolute performance claims, but can certainly compare them

•Compactness. Does Ruler synthesize small rulesets?

•Derivability. Do Ruler’s rulesets derive the rules produced by other methods?

These are answered by the cvc4 evaluation.

•End-to-End. How well do the synthesized rules perform compared to rules generated byexperts in a real application?

Answered by the Herbie evaluation

•Sensitivity Analysis. How do the different components of Ruler’s core algorithm affect perfor-mance and the synthesized rewrite rules?

•Validation Analysis. How do different validation strategies affect Ruler’s output?

Answered by the ablation validation

It should be possible to replicate the evidence 
to substantiate each claim made in the evaluation.
<!-- Add more detail about what we validate -->

# Getting started
Please use the VM provided at 

## Kick the tires
What is the simplest thing we can do?
Bool?
How to check that it worked?

# Step-by-step


## cvc4

## Search ablation
Search ablation aims to examine how different configurations
affect the search. 
In particular, we are interested in the effect of
run-rewrites, particularly when selecting only one rule at a time,
and the effect of changing how many rules are selected.

- How to run
ablation.sh script (choose between pre-gen and your own - long)
- What happens
We run each configuration and collect statistics.
Results availble as pdfs.
- What you might exepct to see
- How to change parameters to get different results
Modify script run.sh

## Validation ablation

# Extending Ruler
