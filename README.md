# OOPSLA 2021 Paper #94 Artifact Overview

This is the artifact for our paper
"Rewrite Rule Inference Using Equality Saturation".
In our paper, we presented a framework, Ruler, 
that uses equality saturation
 to automatically infer small, expressive
 rulesets for a domain,
 given an interpreter.

- **Available**: The artifact is available on Zenodo at: [link]
- **Functional**: Below we first provide instructions for "Getting Started"
  which should take [XXX] minutes.
The next part is "Step-by-Step" which first lists the claims we
  made in the paper and provides instructions on how to validate them.
- **Reusable**: Finally, we also provide instructions on "Further Use / Extending Ruler"
  which describes how to install Ruler
  on a different machine,
  modify the code, and
  perform further experiments.

## Getting started
Please download the `.ova` file [here]
  and open it with Virtual Box by
  going to `File -> import appliance` and giving the path to the `.ova` file
  and clicking on `continue`. In the next window that pops up, click on
  `Import`. It should take a few minutes to import.

* Next, please open the virtual machine image in virtual box by clicking on the
  green `Start` button.

* Login is automatic, but in case needed, the password is: `ruler`.

* The terminal should be open at startup. The project repository is already
  cloned.  Navigate to the `ruler` directory.  All the required packages
  are already installed and Ruler is already compiled for you, ready to be run.

* To allow a quick verification of our artifact,
  we provided pre-generated data and
  results in the VM.
  You can therefore directly view the results (see below on how to do that).

* You can also generate only the plots from the
pre-generated data (see below on how to do that).

* To run the tool yourself entirely from scratch,
first delete all the results directories (as shown below)
and follow the instructions.

### Kick the tires

To check that you are able to run Ruler, type the following in the command line:
```
cargo bool
```
This should take less than a second (when ruler is pre-built, and it is)
  and should generate and print
  5 rewrite rules on the console and the time it took to infer them.

## Step-by-step

Our paper has 4 quantitative evaluations:
- Comparing with CVC4 (`Section 4`): We show that Ruler can infer smaller,
  powerful rulesets faster by comparing the rules inferred for `bool`, `bv4`, and
  `bv32` with varying expression sizes (2, 3). The results are in `Table 1`.

- Integrating with Herbie (`Section 5`): We show that Ruler's rules can 
  be used to replace human-written rules by
  comparing the [Herbie](https://github.com/uwplse/herbie)
  tool's results in fours different configurations: `None`, `Herbie`, `Ruler`, `Both`.
  The results are in `Figure 7`.

- Search Parameter Analyis (`Section 6.1`): We profiled Ruler's search algorithm
  to measure how much time is spent  in each phase. `Figure 8` shows the results for
  `bv4`, `bv32`, and `rationals` domains.
We also compared different variations of `choose_eqs`
  by varying `n` in `Figure 5, Line 3`,
  whose default value is infinity.
The results are shown in `Figure 9a` for `bv4`, `bv32`, and `rationals`.
Importantly, we measure both running time,
  and the number of rules learned.
We also measured running time,
  number of rules learned,
  and number of e-classes in the egraph with and without
  invoking `run_rewrites` (`Figure 4, Line 9`) to study it's effect.
The results are shown in `Figure 9b` for `bv4`, `bv32`, and `rationals`.

- Validation Analysis (`Section 6.2`): We compared different
  rule validation methods for `bv4`, `bv32`, and `rationals`.
The results are shown in `Table 2`.

Below we describe how to run our artifact and reproduce all of them.

### Comparing with CVC4
The goal is to reproduce `Table 1`.
- Type `cd scripts/cvc-eval/` to go to the correct directory.

- To generate the table from the pre-run results,
type `make` and it will print it to the terminal instantly.

- To regenerate the data, type `make clean` to remove all
 pre-generated results and run `make` again.
 This will take approximately 1.5 hours.

Note that the number of rules and the timings
  will not match exactly with what
  we reported in the paper because
  the VM is less powerful than the machine
  we had for running the eval in the paper, and
  the heuristics may have slightly different effects.

#### Additional information about the scripts.
The `Makefile` is the main script for this part.
It runs both Ruler and CVC4 for `bool`, `bv4`, and `bv32` with
2 variables, 3 variables, 2 iterations, and 3 iterations.
It then generates reports in json and uses the
derivability test (`derive.rs`) to compare the proving power of the rulesets
from Ruler and CVC4.
The table is generated by the `compare.py` script and using pandoc and xsv.

### Integrating with Herbie
The goal is to reproduce `Figure 7`.
Herbie is an external tool which we used for this evaluation.
Therefore to avoid any issues that may come up due to Herbie,
  we have uploaded our pre-generated data and the plots that we have in the paper.
- Type `cd scripts/herbie-rational` to go to the correct directory from the `ruler` directory.
- To view the provided plots, go to `output/ruler-herbie-eval/results/2021-04-13-1331-pre-gen`
and open the followin three PDFs
    * for `Figure 7a`: `by-config-all-tests-avg_bits_err_improve-boxplot.pdf`
    * for `Figure 7b`: `by-config-all-tests-output_parens-boxplot.pdf`
    * for `Figure 7c`: `by-config-all-tests-time-boxplot.pdf`

We also provide various other plots that you are welcome to look at! These are
however not presented in the paper and not relevant to this artifact.

- To reproduce all the data
type `./herbie-eval.sh`. This requires racket 7.9 which is already
pre-installed in the directory.
This runs the script with 1 seed by default.
You can run it for fewer or more seeds by typing `./herbie-eval.sh NSEEDS`.
In the paper we ran with 30 seeds but that will take over 10 hours.
We recommend trying with 5 seeds to check the results.
Other arguments are set to default but the script has documentation for how to change them.

#### Additional information about the scripts.
`herbie-eval.sh` is the main script and it has comments to indidate what it does.
Since we are still actively working on Ruler, there
are some scripts that may not be relevant for this part of the evaluation.
Below are the scripts relevant for this eval and a brief description of what they do:

`filter.rkt` filters benchmarks from Herbie that contain only rational operators.
`preprocess.py` preprocesses Ruler's rewrites to make them match with Herbie's syntax,
and also removes expansive directions of rules.
Plotting scripts are in `plots/` directory.
`plots/plot-results.sh` calls these scripts to generate the plots.
`plots/config-all-tests-box-plot.py` is the script that generates the plots in the paper.

### Search Parameter Analysis
The goal is to reproduce `Figure 8` and `Figure 9`.

- Type `cd scripts/ablation` to go to the correct directory.
- To make plots from the pre-generated data,
type `./ablation.sh -r use-existing`.
This will make plots using the data provided in the folder `output/submitted-data` TODO?? is this right?.
It will also print some of the data in the terminal, which we used
for debugging.
Feel free to ingore that.
- To run your own evaluation and make new plots from scratch,
type  `./ablation.sh -r generate-new`.
This will take approximately [XXX] hours.
This runs Ruler with different configurations,
saving each run to its own timestamped folder
under `scripts/ablation/output`,
and then parses the statistics from the log outputs.
These statistics are collected into json files
and then plotted in matplotlib.
Resultant pdf plots are available inside the 
timestamped folder for that experiment.

In the published evaluation,
we ran Ruler with 3 variables,
5 iterations, over 10 runs.
`Figure 8` in the paper corresponds to the `by-domain-phase-times.pdf` plot.
`Figure 9` comprises the rest of the pdf plots.

Note that timing results should only be compared between 
themselves and not as absolute values, since logging is enabled
during the ablation runs.

#### Additional information about the scripts.
`run.sh` controls the entire experiment.
It calls `run-ruler.sh` and `run-ruler-rr.sh` for each domain,
then calls the parsing and visualizing scripts.
`run-ruler.sh` and `run-ruler-rr.sh` will call Ruler 
for a particular domain
with the parameters provided, 
for as many runs as required.
To change the parameters, simply modify the arguments 
passed to `run-ruler.sh` and `run-ruler-rr.sh` 
from inside `run.sh`.
- `-v` is the number of variables,
- `-i` is the iterations,
- `-r` is number of runs (i.e. providing how many independent data points we average over),
- `-d` is the domain
- `-o` is the output folder.
Lastly, any succeeding parameters will be
passed directly to the Ruler invocation.

### Validation Analysis




## Further Use / Extending Ruler
This section describes how to install Ruler on a different machine,
  the required dependencies,
  and how to extend our tool for other domains.

### Dependencies
To install and run the evaluation on a different machine,
  the following dependencies must be installed.

- git
- python3
- rosette 4.0
- cvc4 version 1.8
- herbie (only if you want to reproduce all the results)
- racket
- pandoc
- xsv
- moreutils
- cmake
- curl
- rust
- sudo apt-get install python3-distutils

### Installation
Ruler is implemented in [Rust](rust-lang.org/).
You can install Rust [here](https://www.rust-lang.org/tools/install).
You can then clone the repo and run the tool as described below.

We tested our setup on macOS (Big Sur) and on [LINUXTODO].
To build Ruler, type `cargo build --release`.
This should take ~40 min.

### Usage:
You can generate rules for a `domain` as follows:

```cargo run --bin domain --release -- synth --iters i --variables v```

Type `cargo domain --help` to see all available flags and parameters.


### Project Layout
- The source code resides in the `src` directory.
   * The main algorithm of Ruler is implemented in `lib.rs`.
   *  `equality.rs` defines a rewrite rule.
   * `derive.rs` has the code for running the derivability checks (e.g., see `Sections 4, 5`).
   * `util.rs` has some small helper functions.
   * `convert_sexp.rs` has code that converts the rewrites from CVC4's format to Ruler's format.
   * `bv.rs` has a generic implemenation of bitvectors
      which are specialized to various sizes in `src/bin/`
   * `src/bin/` also contains the implementation of other domains including rationals and bools.
     There are some prototype implementations (floats, strings, bigints)
     that are not evaluated in the paper --- these are work in progress,
     but should give an idea of how to add support for other domains.
     See below for more information on supporting other domains.
- `scripts` has all the scripts used for evaluating Ruler --- each is in a
    designated subdirectory. `compare.py` and `add-arrows.py` are used for the CVC4 comparison.
- `Makefile` is for the CVC4 evaluation.
- `cvc4/` has some of the grammars from CVC4's rule inference tool that we used
   for our evaluation in `Section 4`.
- `results/cvc4/` has the pre-run results from CVC4's rule inference tool.

### Extending Ruler to Support New Domains
Ruler's goal is to support rewrite inference for new domains,
  given a grammar, an interpreter, and a validation technique.
We have already generated documentation for you.
Open `target/doc/ruler/index.html` in your preferred browser to navigate the documentation.

You can generate documentation on your own in a new machine by typing:
```
cargo doc --no-deps
```

To run Ruler with different flags (documentation at `SynthParams.html`)
see the various example usages in `.cargo/config` and try replacing them with other values and look at the results!
For example, you can try
```
cargo run --release --bin rational -- synth --num-fuzz 10 --iters 2
```
to synthesize
rewrite rules for rationals till depth 2,
and by using fuzzing (with 10 values) for rule validation.

To understand how to add support for a new domain,
  you can look at the documentation of the various supported domains like
   `rational` (`target/doc/rational/index.html`),
   `rational_new_div` (`target/doc/rational_new_div/index.html`, relevant for `Section 6.3` in the paper),
   `bool` (`target/doc/bool/index.html`), etc.
Note that some domains (e.g., floats, strings, bigints)
are experimental and not reported in the paper,
but they all provide examples of how you can add support for new domains.

TODO:; rm -rf .git to remove git history and terminal history